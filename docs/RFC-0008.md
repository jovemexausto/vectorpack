# RFC-0001: Branchly — Git Infrastructure as a Managed Layer

| Field | Value |
|---|---|
| RFC | 0001 |
| Title | Branchly Core Architecture |
| Status | Draft |
| Authors | Branchly Core Team |
| Created | 2025-02-27 |
| Last Updated | 2025-02-27 |
| Target Version | 0.1.0 |

---

## Table of Contents

1. [Abstract](#1-abstract)
2. [Motivation](#2-motivation)
3. [Terminology](#3-terminology)
4. [Design Goals](#4-design-goals)
5. [Non-Goals](#5-non-goals)
6. [Background: Git Internals Primer](#6-background-git-internals-primer)
7. [Architecture Overview](#7-architecture-overview)
8. [Core Components](#8-core-components)
   - 8.1 [Git Object Engine](#81-git-object-engine)
   - 8.2 [Storage Abstraction Layer](#82-storage-abstraction-layer)
   - 8.3 [Repository Manager](#83-repository-manager)
   - 8.4 [Tenant Isolation Model](#84-tenant-isolation-model)
   - 8.5 [HTTP Smart Protocol Handler](#85-http-smart-protocol-handler)
   - 8.6 [Auth & Permission Middleware](#86-auth--permission-middleware)
9. [Public API Design](#9-public-api-design)
   - 9.1 [Initialization](#91-initialization)
   - 9.2 [Repository Lifecycle](#92-repository-lifecycle)
   - 9.3 [Commit API](#93-commit-api)
   - 9.4 [Branch API](#94-branch-api)
   - 9.5 [Diff & History API](#95-diff--history-api)
   - 9.6 [HTTP Handler](#96-http-handler)
10. [Storage Backend Specification](#10-storage-backend-specification)
    - 10.1 [Interface Contract](#101-interface-contract)
    - 10.2 [Object Addressing Scheme](#102-object-addressing-scheme)
    - 10.3 [Built-in Adapters](#103-built-in-adapters)
    - 10.4 [Custom Adapters](#104-custom-adapters)
11. [Concurrency & Consistency Model](#11-concurrency--consistency-model)
12. [Pack File & Transfer Protocol](#12-pack-file--transfer-protocol)
13. [Merge Engine](#13-merge-engine)
14. [Caching Architecture](#14-caching-architecture)
15. [Security Model](#15-security-model)
16. [Cloud Migration Path](#16-cloud-migration-path)
17. [Performance Considerations](#17-performance-considerations)
18. [Error Handling Strategy](#18-error-handling-strategy)
19. [Observability](#19-observability)
20. [Versioning & Compatibility](#20-versioning--compatibility)
21. [Open Questions](#21-open-questions)
22. [Alternatives Considered](#22-alternatives-considered)
23. [References](#23-references)

---

## 1. Abstract

Branchly is a TypeScript-native library that exposes Git version control as a composable, storage-agnostic, multi-tenant API layer. Rather than wrapping the Git CLI or spawning child processes, Branchly implements the Git object model natively in JavaScript, backed by a pluggable storage interface. This RFC defines the full technical architecture: the object engine, storage abstraction, tenant isolation model, HTTP smart protocol implementation, concurrency semantics, and the public API surface that developers interact with.

The goal is to allow any backend developer to add version-controlled infrastructure to their SaaS application in under five minutes, without requiring deep knowledge of Git internals, filesystem layout, or pack protocol negotiation.

---

## 2. Motivation

Version control is increasingly needed not just for source code, but as a primitive within SaaS products: design tools need versioned assets, infrastructure platforms need versioned configs, document editors need commit history, low-code tools need branching workflows.

Every team that builds this today faces the same set of problems:

**The shell-wrapping trap.** The most common approach is to shell out to the system `git` binary. This introduces platform dependencies, process-spawning overhead, output-parsing fragility, and severe difficulty isolating data between tenants. Error handling means parsing stderr strings. It is not a foundation you can build a product on.

**The bare repo labyrinth.** Teams that go deeper often manage bare Git repositories on disk, one per tenant. This creates challenges around storage backends (you are now tied to a local filesystem), backup, replication, and horizontal scaling. Moving repos to object storage requires significant custom plumbing.

**Reinvented, differently broken.** Every team reinvents the same primitives — commit creation, ref management, HTTP smart protocol — in slightly different ways. Each implementation carries its own bugs around edge cases in the Git spec: large files, concurrent pushes, pack file deltas, shallow clones.

**No abstraction for tenant isolation.** There is no standard primitive for "a Git repo scoped to a tenant." Every team builds namespace management, access control, and per-tenant quotas from scratch, often insecurely.

Branchly is the abstraction that should exist. Just as ORMs abstract SQL storage and Stripe abstracts payment processing, Branchly abstracts Git infrastructure. The developer calls `repo.commit({ files })`. The plumbing is invisible.

---

## 3. Terminology

| Term | Definition |
|---|---|
| **Object** | A Git content-addressable datum: one of `blob`, `tree`, `commit`, or `tag`. Identified by the SHA-256 hash of its content. |
| **Blob** | A raw file content object. |
| **Tree** | A directory listing object: a sorted list of `(mode, name, sha)` entries pointing to blobs or subtrees. |
| **Commit** | A snapshot object: a pointer to a root tree, zero or more parent commits, author/committer metadata, and a message. |
| **Ref** | A named, mutable pointer to a commit SHA. `refs/heads/main` is a branch tip. `refs/tags/v1.0` is a tag. |
| **HEAD** | A symbolic ref or direct SHA indicating the current working position of a repo. |
| **Pack file** | A binary format for efficiently bundling many Git objects together, using delta compression. Used in clone/push/pull. |
| **Smart HTTP** | Git's HTTP transfer protocol where the server participates in capability negotiation (as opposed to "dumb HTTP", which is read-only static file serving). |
| **Tenant** | The unit of isolation: a user, organization, workspace, or any logical partition. Every repo belongs to exactly one tenant. |
| **Namespace** | The storage path prefix used to isolate a tenant's objects and refs from all others. |
| **Adapter** | A Branchly-defined interface implementation that maps the storage abstraction to a concrete backend (S3, GCS, disk, etc.). |
| **ODB** | Object Database. The component responsible for reading and writing Git objects. |
| **RefDB** | Reference Database. The component responsible for reading, writing, and atomically updating Git refs. |

---

## 4. Design Goals

**G1 — Developer experience is primary.** The API must be intuitive enough that a developer unfamiliar with Git internals can add commit history to their app in under five minutes. Every method name, error message, and type signature should prioritize clarity.

**G2 — Zero system dependencies.** Installing Branchly must not require Git, libgit2, or any native binary on the host system. The entire Git engine runs in pure JavaScript/TypeScript, importable as a standard npm package.

**G3 — Storage portability.** Branchly must be agnostic to the underlying storage medium. The same application code should work identically whether repos are stored on local disk during development, S3 in production, or Branchly Cloud in a managed deployment. Switching backends requires changing one configuration field.

**G4 — Tenant isolation by default.** Multi-tenancy must not be an opt-in feature or an afterthought. The `tenant` parameter is part of the core API contract. Cross-tenant data access must be impossible through the Branchly API, regardless of storage backend.

**G5 — Git wire compatibility.** Repos managed by Branchly must be readable by standard Git clients (`git clone`, `git fetch`, GitHub Desktop, etc.) via the HTTP handler. The pack protocol implementation must be spec-compliant.

**G6 — Correctness over performance.** In cases of conflict between correctness and performance, Branchly must choose correctness. Ref updates must be atomic. Concurrent pushes must not corrupt history.

**G7 — Observable and debuggable.** Branchly must emit structured events and spans for every operation. Integration with OpenTelemetry, custom loggers, and metrics collectors must require no more than a one-line configuration.

**G8 — Smooth cloud migration.** The transition from self-hosted OSS to Branchly Cloud must require no application code changes. It is a configuration swap, not a migration project.

---

## 5. Non-Goals

The following are explicitly out of scope for the initial OSS release:

- **Git LFS (Large File Storage).** Storing binary assets via the LFS pointer protocol is not supported in v0.x. It may be addressed in a future RFC.
- **Submodules.** Branchly does not interpret `.gitmodules` or manage submodule refs.
- **Signed commits (GPG/SSH).** Commit signature verification is not implemented. Signatures in existing objects are preserved but not validated.
- **Garbage collection.** Unreachable object pruning (equivalent to `git gc`) is not implemented in v0.x.
- **Shallow clones.** The `--depth` flag in clone operations is not supported in v0.x.
- **Worktrees.** Branchly has no concept of a checked-out working directory. It operates purely on the object store and refs.
- **Direct disk layout compatibility.** Branchly's on-storage layout does not match `.git/` on disk. Repos cannot be directly `git clone`d from the storage backend without going through the HTTP handler.

---

## 6. Background: Git Internals Primer

Understanding Branchly's architecture requires understanding the Git data model. This section provides a concise reference.

### 6.1 The Object Store

Git's core is a content-addressable object store. An object is stored under a key equal to the SHA-256 hash of its type-prefixed content:

```
key = sha256("blob " + length + "\0" + content)
```

There are four object types:

**Blob** stores raw file bytes with no metadata:
```
blob 13\0Hello, world!
```

**Tree** stores a directory snapshot as a binary-sorted list of entries. Each entry is a tuple of `(mode, name, sha)`:
```
tree 54\0
100644 README.md\0<20-byte-sha>
040000 src\0<20-byte-sha>
```

**Commit** references a root tree and links to parent commits, forming a DAG:
```
commit 183\0
tree 4b825dc642cb6eb9a060e54bf8d69288fbee4904
parent a1b2c3d4...
author Alice <alice@example.com> 1700000000 +0000
committer Alice <alice@example.com> 1700000000 +0000

Initial commit
```

**Tag** is an annotated pointer to any object (usually a commit), with its own message and author.

### 6.2 References

Refs are mutable named pointers stored separately from the object store. A branch (`refs/heads/main`) is simply a file containing a commit SHA. A tag (`refs/tags/v1.0`) is a file containing either a commit SHA (lightweight tag) or a tag object SHA (annotated tag). `HEAD` is a special symbolic ref pointing to the current branch.

### 6.3 The Commit DAG

Commits form a directed acyclic graph. Each commit points to its parent(s). A merge commit has two or more parents. A root commit has none. Branch tips are the newest commits; the full history is the transitive closure of parent pointers from the tip.

### 6.4 Pack Files

Loose objects (one file per SHA) are inefficient at scale. Git periodically packs objects into a binary `.pack` file with delta compression: objects are stored as deltas relative to similar objects, achieving dramatic size reduction. A `.idx` file accompanies each pack for O(log n) object lookup without scanning the pack. The smart HTTP protocol transfers objects as a single negotiated pack file.

---

## 7. Architecture Overview

Branchly is structured as four layers, each with a well-defined interface to the layer above and below:

```
┌─────────────────────────────────────────────────────────────────┐
│                        Public API Layer                         │
│   Branchly class · RepoHandle · BranchHandle · CommitHandle    │
└──────────────────────────────┬──────────────────────────────────┘
                               │
┌──────────────────────────────▼──────────────────────────────────┐
│                       Repository Layer                          │
│     RepositoryManager · TenantNamespace · RefDB · AuthCtx      │
└──────────────────────────────┬──────────────────────────────────┘
                               │
┌──────────────────────────────▼──────────────────────────────────┐
│                       Git Engine Layer                          │
│      ObjectDB · TreeBuilder · CommitBuilder · PackEngine       │
│      DiffEngine · MergeEngine · HistoryWalker                  │
└──────────────────────────────┬──────────────────────────────────┘
                               │
┌──────────────────────────────▼──────────────────────────────────┐
│                     Storage Adapter Layer                       │
│   StorageAdapter interface · S3Adapter · GCSAdapter ·          │
│   DiskAdapter · MemoryAdapter · BranchlyCloudAdapter           │
└─────────────────────────────────────────────────────────────────┘
```

**Public API Layer** is the developer-facing surface. It exposes high-level, intent-oriented methods like `repo.commit()`, `repo.branch()`, and `repo.diff()`. All Git concepts are either abstracted away or surfaced as typed, structured objects rather than raw strings.

**Repository Layer** manages the lifecycle and metadata of repos: creation, deletion, access control, ref management, and tenant namespacing. It is the gatekeeper between the public API and the engine.

**Git Engine Layer** is the Git object model implementation. It creates, reads, and computes Git objects without any filesystem or storage concern. It is pure algorithmic code: computing pack files, tree diffs, 3-way merges, and commit graph traversals.

**Storage Adapter Layer** implements the physical read/write operations for objects and refs. All adapters implement the same `StorageAdapter` interface. The engine never calls storage directly; it always goes through this interface.

---

## 8. Core Components

### 8.1 Git Object Engine

The Git Object Engine (`ObjectDB`) is a pure-JS implementation of Git's object read/write semantics. It has no storage concern and no side effects beyond computing hashes and serializing/deserializing object data.

#### Object Serialization

Each object type is serialized to the canonical Git format before hashing:

```typescript
function serializeBlob(content: Uint8Array): Uint8Array {
  const header = encode(`blob ${content.length}\0`)
  return concat(header, content)
}

function serializeTree(entries: TreeEntry[]): Uint8Array {
  // entries sorted lexicographically by name, per Git spec
  const sorted = [...entries].sort((a, b) => gitTreeSortKey(a) < gitTreeSortKey(b) ? -1 : 1)
  return concat(...sorted.map(e => concat(
    encode(`${e.mode} ${e.name}\0`),
    hexToBytes(e.sha)
  )))
}

function serializeCommit(commit: CommitData): Uint8Array {
  const lines = [
    `tree ${commit.tree}`,
    ...commit.parents.map(p => `parent ${p}`),
    `author ${formatSig(commit.author)}`,
    `committer ${formatSig(commit.committer)}`,
    '',
    commit.message,
  ]
  return encode(lines.join('\n'))
}
```

#### Object Hashing

Branchly targets SHA-256 (the format used by `git init --object-format=sha256`, the modern Git default) while also supporting SHA-1 for compatibility with existing repos. The hash function is configurable at the `Branchly` constructor level.

```typescript
async function hashObject(data: Uint8Array): Promise<string> {
  const hashBuffer = await crypto.subtle.digest('SHA-256', data)
  return bufferToHex(hashBuffer)
}
```

`crypto.subtle` is used for all hashing, ensuring compatibility with both Node.js and edge runtimes (Cloudflare Workers, Deno, Bun) without native dependencies.

#### Tree Building

The `TreeBuilder` constructs a root tree object from a flat list of file paths. It recursively creates intermediate tree objects for nested directories:

```typescript
class TreeBuilder {
  async buildFromFiles(files: FileEntry[], odb: ObjectDB): Promise<string> {
    // Group files by top-level directory component
    const root = groupByDirectory(files)
    return this.buildTree(root, odb)
  }

  private async buildTree(node: DirNode, odb: ObjectDB): Promise<string> {
    const entries: TreeEntry[] = []

    for (const [name, child] of node.dirs) {
      const treeSha = await this.buildTree(child, odb)
      entries.push({ mode: '040000', name, sha: treeSha })
    }

    for (const [name, file] of node.files) {
      const blobSha = await odb.writeBlob(file.content)
      entries.push({ mode: file.executable ? '100755' : '100644', name, sha: blobSha })
    }

    const treeData = serializeTree(entries)
    return odb.writeObject('tree', treeData)
  }
}
```

### 8.2 Storage Abstraction Layer

The `StorageAdapter` is a minimal interface that all backends must implement. It is deliberately low-level: it stores and retrieves raw bytes by key, and provides atomic compare-and-swap for ref updates. All higher-level semantics live in the engine.

```typescript
interface StorageAdapter {
  // Object store: content-addressed, immutable once written
  getObject(namespace: string, sha: string): Promise<Uint8Array | null>
  putObject(namespace: string, sha: string, data: Uint8Array): Promise<void>
  hasObject(namespace: string, sha: string): Promise<boolean>
  listObjects(namespace: string, prefix?: string): AsyncIterable<string>

  // Ref store: named mutable pointers
  getRef(namespace: string, refName: string): Promise<string | null>
  setRef(namespace: string, refName: string, sha: string): Promise<void>
  deleteRef(namespace: string, refName: string): Promise<void>
  listRefs(namespace: string, prefix?: string): AsyncIterable<RefEntry>

  // Atomic compare-and-swap for concurrency safety
  compareAndSwapRef(
    namespace: string,
    refName: string,
    expectedSha: string | null,
    newSha: string,
  ): Promise<{ success: boolean; actual: string | null }>
}
```

The separation between object operations and ref operations is intentional. Objects are immutable and content-addressed — they can be written concurrently without conflict. Refs are mutable and require atomic update semantics to prevent corruption under concurrent pushes.

### 8.3 Repository Manager

The `RepositoryManager` is the central orchestrator. It manages repo metadata, maps `(tenant, repoName)` pairs to storage namespaces, and vends `RepoHandle` instances to callers.

```typescript
class RepositoryManager {
  constructor(
    private readonly adapter: StorageAdapter,
    private readonly engine: GitEngine,
    private readonly config: BranchlyConfig,
  ) {}

  async createRepo(params: CreateRepoParams): Promise<RepoHandle> {
    const namespace = this.resolveNamespace(params.tenant, params.name)

    // Initialize with an empty root commit on the default branch
    const emptyTreeSha = await this.engine.odb.writeTree([])
    const initCommit = await this.engine.createCommit({
      tree: emptyTreeSha,
      parents: [],
      message: 'Initial commit',
      author: BRANCHLY_SYSTEM_AUTHOR,
    })

    await this.adapter.setRef(namespace, `refs/heads/${params.defaultBranch ?? 'main'}`, initCommit)
    await this.adapter.setRef(namespace, 'HEAD', `ref: refs/heads/${params.defaultBranch ?? 'main'}`)

    return new RepoHandle(namespace, this.adapter, this.engine)
  }

  async getRepo(params: GetRepoParams): Promise<RepoHandle | null> {
    const namespace = this.resolveNamespace(params.tenant, params.name)
    const head = await this.adapter.getRef(namespace, 'HEAD')
    if (!head) return null
    return new RepoHandle(namespace, this.adapter, this.engine)
  }

  private resolveNamespace(tenant: string, repoName: string): string {
    // Sanitize and combine into a storage path prefix
    return `${sanitize(tenant)}/${sanitize(repoName)}`
  }
}
```

### 8.4 Tenant Isolation Model

Tenant isolation is enforced at the namespace level. Every storage operation in Branchly carries a `namespace` string that is derived deterministically from `(tenant, repoName)`. The storage adapter only ever accesses keys within that namespace.

The namespace forms the key prefix for all objects and refs:

```
Objects:  {tenant}/{repo}/objects/{sha[0:2]}/{sha[2:]}
Refs:     {tenant}/{repo}/refs/heads/main
          {tenant}/{repo}/refs/tags/v1.0
          {tenant}/{repo}/HEAD
Metadata: {tenant}/{repo}/.branchly/meta.json
```

**Namespace collision prevention.** Tenant strings are sanitized before use: they must match `[a-zA-Z0-9_\-]+`. Any special characters are percent-encoded or rejected. The sanitization ensures that a malicious tenant ID like `../../other-tenant` cannot traverse into another tenant's namespace.

**API-level enforcement.** `RepoHandle` instances carry their namespace and cannot be constructed without going through `RepositoryManager`. The manager enforces that the calling auth context has permission for the requested tenant before issuing a handle (see §8.6).

**Storage-level enforcement.** For cloud deployments, the `BranchlyCloudAdapter` additionally enforces namespace isolation via server-side policy, so a compromised client SDK cannot access objects outside its authenticated scope.

### 8.5 HTTP Smart Protocol Handler

Git's smart HTTP protocol allows any standard Git client to `clone`, `fetch`, and `push` from a Branchly-managed repo over HTTP. The `httpHandler()` method on a `RepoHandle` returns a standard WHATWG `Request → Response` handler, making it embeddable in any Node.js, Bun, or edge runtime.

The smart HTTP protocol consists of two phases:

**Discovery phase** (`GET /info/refs?service=git-upload-pack` or `git-receive-pack`):

The client requests a list of the server's refs and capabilities. The server responds with a pkt-line formatted list of all refs and a capability string.

```typescript
async function handleInfoRefs(repo: RepoHandle, service: GitService): Promise<Response> {
  const refs = await repo.listRefs()
  const lines: string[] = [`# service=${service}\n`, null /* flush */]

  if (refs.length === 0) {
    lines.push(`0000000000000000000000000000000000000000 capabilities^{}\0${CAPABILITIES}`)
  } else {
    const [first, ...rest] = refs
    lines.push(`${first.sha} ${first.name}\0${CAPABILITIES}`)
    rest.forEach(r => lines.push(`${r.sha} ${r.name}`))
  }

  return new Response(encodePktLines(lines), {
    headers: {
      'Content-Type': `application/x-${service}-advertisement`,
      'Cache-Control': 'no-cache',
    },
  })
}
```

**Transfer phase** (`POST /git-upload-pack` for fetch/clone, `/git-receive-pack` for push):

For `upload-pack` (serving a clone/fetch), the client sends a list of "want" SHAs (commits it wants) and "have" SHAs (commits it already has). The server computes the minimal set of objects the client needs, packs them, and streams the pack:

```typescript
async function handleUploadPack(repo: RepoHandle, body: ReadableStream): Promise<Response> {
  const { wants, haves } = await parseUploadPackRequest(body)
  const commonBase = await findCommonAncestors(repo, wants, haves)
  const objectsToSend = await collectReachableObjects(repo, wants, commonBase)
  const pack = await repo.engine.packObjects(objectsToSend)

  return new Response(
    encodeSidebandStream(pack),
    { headers: { 'Content-Type': 'application/x-git-upload-pack-result' } }
  )
}
```

For `receive-pack` (accepting a push), the server receives a pack, validates it, and atomically updates the pushed refs:

```typescript
async function handleReceivePack(repo: RepoHandle, body: ReadableStream, auth: AuthContext): Promise<Response> {
  const { refUpdates, pack } = await parseReceivePackRequest(body)

  // Authorize each ref update
  for (const update of refUpdates) {
    await auth.assertCanWrite(repo.namespace, update.refName)
  }

  // Write all received objects to the ODB
  await repo.engine.unpackObjects(pack, repo.namespace)

  // Atomically update refs using compare-and-swap
  const results = await Promise.all(refUpdates.map(u =>
    repo.adapter.compareAndSwapRef(repo.namespace, u.refName, u.oldSha, u.newSha)
  ))

  return new Response(encodeReceivePackResult(results), {
    headers: { 'Content-Type': 'application/x-git-receive-pack-result' }
  })
}
```

### 8.6 Auth & Permission Middleware

Branchly does not own authentication — it delegates entirely to the host application. The `auth` option on the constructor accepts a callback that receives the incoming request and must return an `AuthContext`:

```typescript
interface AuthContext {
  // Which tenants this caller may access
  allowedTenants: string[] | '*'

  // Granular repo-level permissions
  canRead(namespace: string): boolean
  canWrite(namespace: string): boolean
  canAdmin(namespace: string): boolean
}

// Usage: plug in your existing session/JWT logic
const git = new Branchly({
  storage: adapter,
  auth: async (req: Request): Promise<AuthContext> => {
    const session = await verifyJWT(req.headers.get('Authorization'))
    return {
      allowedTenants: [session.userId],
      canRead: (ns) => ns.startsWith(session.userId),
      canWrite: (ns) => ns.startsWith(session.userId),
      canAdmin: (ns) => session.role === 'admin',
    }
  }
})
```

The `AuthContext` is threaded through every operation on `RepoHandle`. If a caller attempts to access a namespace outside their `allowedTenants`, Branchly throws a `BranchlyPermissionError` before any storage call is made.

---

## 9. Public API Design

All public API methods are async and return typed result objects. They throw typed `BranchlyError` subclasses on failure rather than returning error codes. The API is designed to be discoverable via TypeScript IntelliSense.

### 9.1 Initialization

```typescript
import { Branchly } from 'branchly'
import { S3Adapter } from 'branchly/adapters/s3'

const git = new Branchly({
  // Required: storage backend
  storage: new S3Adapter({
    bucket: 'my-repos',
    region: 'us-east-1',
    prefix: 'repos/',
  }),

  // Optional: authentication callback (defaults to allow-all)
  auth: async (req) => myAuthMiddleware(req),

  // Optional: default branch name for new repos
  defaultBranch: 'main',

  // Optional: hash algorithm ('sha256' | 'sha1', default: 'sha256')
  objectFormat: 'sha256',

  // Optional: cache configuration
  cache: {
    driver: 'memory',
    maxSizeMb: 256,
    ttlSeconds: 300,
  },

  // Optional: telemetry
  telemetry: {
    tracer: opentelemetryTracer,
    logger: pinoLogger,
  },
})
```

### 9.2 Repository Lifecycle

```typescript
// Create a new repo for a tenant
const repo = await git.createRepo({
  tenant: 'user_abc123',
  name: 'my-project',
  description: 'Optional description',
  defaultBranch: 'main',      // optional, inherits global default
  visibility: 'private',      // for cloud; ignored in OSS
})

// Open an existing repo
const repo = await git.getRepo({
  tenant: 'user_abc123',
  name: 'my-project',
})
// Returns null if the repo does not exist

// List all repos for a tenant
const repos = await git.listRepos({ tenant: 'user_abc123' })
// Returns: Array<{ name, description, defaultBranch, createdAt, updatedAt, size }>

// Delete a repo (irreversible)
await git.deleteRepo({
  tenant: 'user_abc123',
  name: 'my-project',
  confirm: true,  // required safety flag
})
```

### 9.3 Commit API

```typescript
// Commit a set of files to the default branch
const commit = await repo.commit({
  message: 'Add README and initial config',
  author: {
    name: 'Alice',
    email: 'alice@example.com',
    // timestamp defaults to Date.now() if omitted
  },
  files: [
    { path: 'README.md', content: '# My Project\n' },
    { path: 'config/app.json', content: JSON.stringify(config) },
    { path: 'scripts/deploy.sh', content: script, executable: true },
  ],
  // Optional: base branch (defaults to repo's HEAD)
  branch: 'main',
})

// commit is a CommitHandle:
console.log(commit.sha)        // 'a1b2c3d4...'
console.log(commit.shortSha)   // 'a1b2c3d'
console.log(commit.timestamp)  // Date
console.log(commit.message)    // 'Add README and initial config'
console.log(commit.parents)    // string[] of parent SHAs

// Commit with file deletions and updates
const commit2 = await repo.commit({
  message: 'Refactor config',
  author: { name: 'Alice', email: 'alice@example.com' },
  files: [
    { path: 'config/app.json', content: JSON.stringify(newConfig) },  // update
  ],
  delete: ['config/old-config.json'],  // delete files
})

// Read file content at any commit
const content = await repo.readFile({
  path: 'README.md',
  ref: 'main',         // branch name, tag, or SHA
})
// Returns: { content: string, sha: string, mode: string }

// List files at a tree path
const tree = await repo.listFiles({
  path: 'config/',
  ref: 'main',
})
// Returns: Array<{ path, sha, mode, size }>
```

### 9.4 Branch API

```typescript
// Create a branch from a commit or another branch
const branch = await repo.branch('feature/new-ui')
// Defaults to branching from HEAD

const branch2 = await repo.branch('hotfix/login', { from: 'v1.2.3' })
// Can branch from any ref or SHA

// List branches
const branches = await repo.listBranches()
// Returns: Array<{ name, sha, isDefault, updatedAt }>

// Delete a branch
await repo.deleteBranch('feature/old-experiment')

// Rename a branch
await repo.renameBranch('master', 'main')

// Get branch handle for chained operations
const branch = await repo.getBranch('feature/new-ui')
const commit = await branch.commit({ message: '...', files: [...] })
```

### 9.5 Diff & History API

```typescript
// Diff between two refs
const diff = await repo.diff('main', 'feature/new-ui')
// Returns: DiffResult
// {
//   files: Array<{
//     path: string
//     status: 'added' | 'modified' | 'deleted' | 'renamed'
//     oldPath?: string       // for renames
//     additions: number
//     deletions: number
//     hunks: DiffHunk[]     // unified diff hunks
//   }>
//   totalAdditions: number
//   totalDeletions: number
// }

// Diff a single file
const fileDiff = await repo.diffFile({
  path: 'config/app.json',
  base: 'main',
  head: 'feature/new-ui',
})

// Commit history / log
const log = await repo.log({
  ref: 'main',
  limit: 50,
  since: new Date('2024-01-01'),
  path: 'src/',   // optional: only commits touching this path
})
// Returns: CommitHandle[]

// Get a single commit
const commit = await repo.getCommit('a1b2c3d4')

// Get annotated blame for a file
const blame = await repo.blame({
  path: 'src/index.ts',
  ref: 'main',
})
// Returns: Array<{ line: number, content: string, commit: CommitHandle }>

// Restore a file to a previous version
await repo.restore({
  path: 'config/app.json',
  ref: 'a1b2c3d4',
  branch: 'main',
  message: 'Revert config to a1b2c3d',
})
```

### 9.6 HTTP Handler

```typescript
import express from 'express'

const app = express()

// Mount the Branchly HTTP handler at a path
// The handler is framework-agnostic: it accepts a WHATWG Request and returns a Response
app.use('/git/:tenant/:repo', async (req, res) => {
  const repo = await git.getRepo({
    tenant: req.params.tenant,
    name: req.params.repo,
  })

  if (!repo) return res.status(404).end()

  const response = await repo.httpHandler()(toWHATWGRequest(req))
  sendWHATWGResponse(response, res)
})

// Or use the built-in Express/Hono/Fastify adapters:
app.use('/git', git.expressRouter())      // Express
app.route('/git/*').all(git.honoHandler()) // Hono
```

---

## 10. Storage Backend Specification

### 10.1 Interface Contract

Implementations of `StorageAdapter` must satisfy the following invariants:

**Objects are immutable.** Once `putObject` succeeds for a given SHA, subsequent calls to `getObject` with the same SHA must return the same bytes. Implementations may deduplicate writes (if the object already exists, `putObject` is a no-op), but must never return different bytes for the same SHA.

**`getObject` returns null for missing objects.** It must never throw a "not found" error; that would require callers to catch errors for a normal control flow.

**`compareAndSwapRef` is the only safe way to update refs.** Direct `setRef` calls are provided for initialization and administrative use (repo creation, migrations). All code paths that could run concurrently must use `compareAndSwapRef`.

**`listObjects` and `listRefs` may be eventually consistent.** On backends like S3 where LIST operations have eventual consistency, it is acceptable for these iterables to miss recently written objects. The engine never relies on list results for correctness — only for optimization (e.g., collecting objects for a pack).

### 10.2 Object Addressing Scheme

Objects are stored at keys derived from their SHA as follows:

```
{namespace}/objects/{sha[0:2]}/{sha[2:]}
```

This two-character prefix sharding is adopted from Git's own `.git/objects/` layout. On S3 and similar key-value stores, this improves parallelism for batch reads by distributing keys across multiple partitions.

Refs are stored at:

```
{namespace}/refs/heads/{branchName}
{namespace}/refs/tags/{tagName}
{namespace}/HEAD
```

Pack files (for cloud deployments with pack-file caching) are stored at:

```
{namespace}/.packs/{sha}.pack
{namespace}/.packs/{sha}.idx
```

Repo metadata is stored at:

```
{namespace}/.branchly/meta.json
```

### 10.3 Built-in Adapters

**`DiskAdapter`** stores objects as files on the local filesystem. Best for development, single-server deployments, and testing.

```typescript
new DiskAdapter({ basePath: '/var/repos' })
```

Uses Node.js `fs/promises`. Implements `compareAndSwapRef` via a `.lock` file pattern (create lock, write, rename) which is atomic on POSIX filesystems but not on Windows.

**`MemoryAdapter`** stores all data in a JavaScript `Map`. Used for testing, ephemeral sandboxes, and integration tests. Not persistent across process restarts.

```typescript
new MemoryAdapter()
```

Implements `compareAndSwapRef` via JavaScript's single-threaded event loop (no true concurrency, so CAS is trivially safe).

**`S3Adapter`** stores objects in an Amazon S3 bucket (or any S3-compatible store: MinIO, R2, Backblaze B2).

```typescript
new S3Adapter({
  bucket: 'my-repos',
  region: 'us-east-1',
  prefix: 'repos/',
  // Optional: supply your own @aws-sdk/client-s3 instance
  client: new S3Client({ ... }),
})
```

Implements `compareAndSwapRef` via S3's conditional writes: `PutObject` with `If-None-Match: *` for new refs, and a combination of `GetObject` + `PutObject` with `If-Match: <etag>` for updates. Note: S3 does not provide a native atomic CAS — this is approximated using ETags with an optimistic retry loop. See §11 for concurrency implications.

**`GCSAdapter`** stores objects in Google Cloud Storage.

```typescript
new GCSAdapter({
  bucket: 'my-repos',
  prefix: 'repos/',
})
```

GCS supports `ifGenerationMatch` for conditional writes, enabling true atomic CAS semantics.

**`BranchlyCloudAdapter`** delegates to Branchly's managed API. Identical interface; no local object storage.

```typescript
new BranchlyCloudAdapter({
  apiKey: process.env.BRANCHLY_API_KEY,
})
```

### 10.4 Custom Adapters

Any JavaScript object implementing the `StorageAdapter` interface is a valid adapter. This enables integration with non-standard backends:

```typescript
// Example: DynamoDB-backed adapter
class DynamoAdapter implements StorageAdapter {
  async getObject(namespace, sha) { /* ... */ }
  async putObject(namespace, sha, data) { /* ... */ }
  // ... etc
}

const git = new Branchly({ storage: new DynamoAdapter({ tableName: 'repos' }) })
```

Adapters are encouraged to implement the optional `StorageAdapterCapabilities` interface to declare their CAS semantics, consistency model, and max object size, allowing Branchly to choose optimal code paths.

---

## 11. Concurrency & Consistency Model

### 11.1 Object Writes

Git object writes are inherently safe to parallelize. Because the SHA is computed from the content, two concurrent writes of the same file produce the same SHA and are idempotent. `putObject` implementations must be safe to call concurrently for the same SHA.

### 11.2 Ref Updates (the critical path)

Concurrent pushes to the same branch are the primary concurrency hazard. If two clients both read `refs/heads/main` as `sha-A` and both attempt to push new commits on top of `sha-A`, only one can win — the other must fail with a non-fast-forward error.

Branchly uses `compareAndSwapRef` for all ref updates arising from pushes and merges:

```
T=0: Client 1 reads refs/heads/main → sha-A
T=0: Client 2 reads refs/heads/main → sha-A
T=1: Client 1 calls CAS(main, expected=sha-A, new=sha-B) → SUCCESS
T=2: Client 2 calls CAS(main, expected=sha-A, new=sha-C) → FAIL (actual=sha-B)
T=3: Client 2 receives BranchlyRefConflictError
```

The client must re-fetch, rebase or merge their changes against `sha-B`, then retry the push with a new commit.

### 11.3 CAS Approximation on S3

S3 does not offer true compare-and-swap on `PutObject`. Branchly approximates CAS on S3 using a two-phase protocol:

1. Read the current ETag of the ref key with `HeadObject`.
2. Attempt `PutObject` with `If-Match: <etag>`.
3. S3 returns `PreconditionFailed` if the ETag has changed between steps 1 and 2.

This provides strong safety (a concurrent writer is always detected) but not strict atomicity (there is a window between step 1 and step 2 where a third writer could interleave). For high-concurrency deployments, the `S3Adapter` supports an optional `lockBackend` (e.g., DynamoDB or Redis) for true distributed locking.

### 11.4 Pack File Atomicity

When writing a pack file, Branchly writes the `.pack` file first, then the `.idx` file. A reader that sees an `.idx` is guaranteed to find its corresponding `.pack`. A partially written pack (`.pack` without `.idx`) is treated as non-existent.

---

## 12. Pack File & Transfer Protocol

### 12.1 Pack File Format

A pack file is a binary container for multiple Git objects. The format is:

```
[4 bytes]  Magic: PACK
[4 bytes]  Version: 2
[4 bytes]  Number of objects (big-endian uint32)
[N × entry] Object entries (type + size + data)
[20 bytes] SHA of all preceding bytes
```

Each object entry encodes its type and uncompressed size in a variable-length integer, followed by zlib-compressed content (for "undeltified" objects) or a delta instruction stream (for "deltified" objects referencing another object in the pack).

### 12.2 Pack Generation

When serving a `git fetch` or `git clone`, Branchly generates a pack file containing exactly the objects needed by the client. The algorithm:

1. **Commit graph traversal.** Walk from each "want" SHA back through parent pointers until hitting a "have" SHA or a root commit. Collect all reachable commit SHAs.

2. **Object enumeration.** For each reachable commit, collect its root tree SHA. Recursively enumerate all tree and blob SHAs. Deduplicate.

3. **Delta compression.** For similar blobs (same path across different commits, or blobs with high byte-similarity), compute binary deltas using a sliding-window diff. Store the smaller representation (delta or full content).

4. **Pack serialization.** Write the pack header, serialize each object entry, append the pack checksum.

For the v0.1.0 release, step 3 uses a simplified delta algorithm that only considers consecutive versions of the same file path. Full cross-object delta search (as Git's `git pack-objects` implements) is deferred to a later version.

### 12.3 Pack Unpacking

On `git push`, the client sends a pack file containing the new objects. Branchly unpacks it by:

1. Reading the pack header to determine object count.
2. Iterating entries. For undeltified objects, decompress and write directly to the ODB. For deltified objects, resolve the base object (potentially from the ODB or from earlier in the pack), apply the delta, and write the result.
3. Verifying the pack checksum.
4. Verifying that all "want" SHAs referenced in the ref update commands are present in the ODB after unpacking.

---

## 13. Merge Engine

### 13.1 Merge Strategy

Branchly implements a recursive three-way merge, consistent with Git's default `ort` strategy.

Given branches A and B to merge:

1. **Find the merge base.** Walk the commit DAG to find the most recent common ancestor commit C of A and B.
2. **Compute diffs.** Diff C→A and C→B at the file level.
3. **Classify each file.** For each path touched by either diff:
   - If only one side changed it: take that side's version (no conflict).
   - If both sides changed it identically: take either version (no conflict).
   - If both sides changed it differently: attempt a line-level three-way merge. If line-level merge succeeds (no overlapping hunks), produce a merged file. If overlapping hunks exist, emit a conflict.
4. **Handle renames.** Files renamed on one side are tracked by content similarity (Jaccard index on 4-gram shingles) and matched to their counterpart on the other side.
5. **Produce result.** If no conflicts, create a merge commit with two parents (A and B) pointing to the merged tree. If conflicts, return a `BranchlyMergeConflictError` with the per-file conflict details.

### 13.2 Conflict Representation

```typescript
interface MergeConflict {
  path: string
  type: 'content' | 'add-add' | 'modify-delete' | 'rename-rename'
  ours: string    // Content with conflict markers (<<<, ===, >>>)
  theirs: string  // Their version
  base: string    // Common ancestor version
}
```

The caller receives this structured conflict data and can either present it to the end user for resolution, auto-resolve using a custom strategy (e.g., always take `ours`), or write the conflict markers to storage for manual resolution.

---

## 14. Caching Architecture

Branchly operates on a principle of **read-heavy workloads**. Commit history reads, file content fetches, and clone operations are far more frequent than commits. A caching layer is essential for acceptable latency on non-disk backends.

### 14.1 Object Cache

Objects are immutable once written, making them ideal cache entries. The object cache stores deserialized objects keyed by SHA:

```typescript
interface ObjectCache {
  get(sha: string): CachedObject | undefined
  set(sha: string, obj: CachedObject): void
  delete(sha: string): void
}
```

The default in-process LRU cache (configurable size in MB) stores blobs, trees, and commits. Blobs over a configurable size threshold (`maxBlobCacheSizeBytes`, default 512KB) are excluded.

For distributed deployments, a Redis cache adapter is provided:

```typescript
new Branchly({
  storage: s3Adapter,
  cache: {
    driver: 'redis',
    client: redisClient,
    ttlSeconds: 3600,
    maxBlobSizeBytes: 256_000,
  }
})
```

### 14.2 Ref Cache

Refs are mutable and cannot be cached with a long TTL. The ref cache uses short TTLs (default: 5 seconds for reads, immediately invalidated on write) and is primarily useful for reducing storage round-trips under high read fan-out (e.g., many clients polling for HEAD updates).

### 14.3 Pack Cache

For repos with large history, Branchly can cache pre-computed pack files for common fetch patterns (e.g., a full clone pack). Pack cache entries are keyed by `(namespace, tipSha)` and invalidated on push.

---

## 15. Security Model

### 15.1 Threat Model

Branchly's primary security concern is **cross-tenant data access**: a user of tenant A must never be able to read or write objects belonging to tenant B, regardless of how they interact with the API.

Secondary concerns:

- **Path traversal in namespace resolution.** A malicious tenant ID must not allow escaping the namespace.
- **Denial of service via large objects.** Unbounded object writes could exhaust storage or memory.
- **Pack bomb attacks.** A maliciously crafted pack file with extreme delta chains could exhaust CPU/memory during unpacking.

### 15.2 Namespace Sanitization

Tenant IDs and repo names are validated against `^[a-zA-Z0-9_\-]{1,128}$` before use. Any string failing this regex causes a `BranchlyValidationError` before any storage interaction.

The concatenated namespace (`{tenant}/{repo}`) is additionally checked to not contain `..` components after joining, providing defense in depth against path traversal even if validation is somehow bypassed.

### 15.3 Object Size Limits

Configurable limits prevent resource exhaustion:

```typescript
new Branchly({
  limits: {
    maxBlobSizeBytes: 100 * 1024 * 1024,    // 100MB per file
    maxCommitFilesCount: 10_000,             // files per commit
    maxRepoSizeBytes: 10 * 1024 * 1024 * 1024, // 10GB per repo
    maxPackSizeBytes: 500 * 1024 * 1024,    // 500MB per push
  }
})
```

### 15.4 Pack Bomb Mitigation

During pack unpacking, Branchly enforces:

- Maximum delta chain depth: 50 (Git's default is also ~50).
- Maximum uncompressed object size: equal to `maxBlobSizeBytes`.
- Maximum total unpacked size per push: equal to `maxPackSizeBytes`.

If any limit is exceeded, unpacking is aborted and the push is rejected.

### 15.5 Auth in the HTTP Handler

The HTTP handler authenticates incoming requests using the `auth` callback provided at construction. The callback receives the raw `Request` object, allowing the host application to verify JWTs, session cookies, API keys, or any other credential scheme. If the callback throws or returns null, the handler responds with `401 Unauthorized`.

---

## 16. Cloud Migration Path

A core design principle of Branchly is that migrating from self-hosted OSS to Branchly Cloud requires changing exactly one line of code: the storage adapter.

```typescript
// Before: self-hosted with S3
const git = new Branchly({
  storage: new S3Adapter({ bucket: 'my-repos', region: 'us-east-1' }),
})

// After: Branchly Cloud (zero other changes)
const git = new Branchly({
  storage: new BranchlyCloudAdapter({ apiKey: process.env.BRANCHLY_API_KEY }),
})
```

Everything else — `createRepo`, `commit`, `branch`, `diff`, `httpHandler` — is identical. The cloud adapter implements the same `StorageAdapter` interface, backed by Branchly's managed infrastructure.

### 16.1 Data Migration

For teams migrating existing repos from self-hosted storage to Branchly Cloud, the `branchly migrate` CLI command handles the transfer:

```bash
branchly migrate \
  --from s3://my-bucket/repos \
  --to branchly://my-org \
  --api-key $BRANCHLY_API_KEY
```

The migration is performed object-by-object with deduplication. It is safe to run while the source is live; any objects written after migration begins are tracked via a watermark and replayed in a final catch-up pass.

---

## 17. Performance Considerations

### 17.1 Commit Latency

A single `repo.commit()` call requires the following storage operations:

1. Read `HEAD` ref — 1 read
2. Read current commit object — 1 read
3. Read current root tree (for incremental tree update) — 1 read + N reads for affected subtrees
4. Write new blob objects (one per changed file) — M writes
5. Write new tree objects (one per affected directory) — K writes (K ≤ depth × changed files)
6. Write new commit object — 1 write
7. Update `HEAD` ref via CAS — 1 CAS

For a commit touching F files at a maximum directory depth D, the total storage operations are roughly `3 + F + F×D + 1`. For a flat repo with 10 changed files, this is ~33 operations. On S3 with ~10ms per operation and 10 concurrent requests, commit latency is approximately 50–100ms.

### 17.2 Clone Latency

Clone performance is dominated by pack generation (CPU) and transfer (network). For the initial pack-file cache miss, generating a pack for a 1,000-commit repo with 500 files takes approximately 200–500ms of CPU time plus storage read time. Cached packs reduce this to a single storage read.

### 17.3 Read Path Optimization

For `repo.readFile()` and `repo.listFiles()`, Branchly optimizes the read path to avoid unnecessary object fetches:

- If the file's SHA is already in the object cache, no storage read is needed.
- Tree objects are cached aggressively, as they are shared across many commits.
- For `listFiles()`, Branchly returns tree entry metadata (SHA, mode, size for blobs) without fetching blob content.

---

## 18. Error Handling Strategy

All Branchly errors extend `BranchlyError` and carry structured metadata for programmatic handling:

```typescript
class BranchlyError extends Error {
  readonly code: BranchlyErrorCode
  readonly namespace?: string
  readonly ref?: string
  readonly sha?: string
}

// Specific error types:
class BranchlyNotFoundError extends BranchlyError {}       // Repo, commit, or file not found
class BranchlyPermissionError extends BranchlyError {}     // Auth check failed
class BranchlyRefConflictError extends BranchlyError {     // Non-fast-forward push
  readonly expectedSha: string
  readonly actualSha: string
}
class BranchlyMergeConflictError extends BranchlyError {   // Merge produced conflicts
  readonly conflicts: MergeConflict[]
}
class BranchlyValidationError extends BranchlyError {}     // Invalid input
class BranchlyStorageError extends BranchlyError {         // Adapter-level failure
  readonly cause: Error
}
class BranchlyLimitError extends BranchlyError {           // Size/count limit exceeded
  readonly limit: string
  readonly actual: number
  readonly max: number
}
```

Errors are never swallowed. Storage adapter errors are always wrapped in `BranchlyStorageError` to distinguish infrastructure failures from logic errors.

---

## 19. Observability

### 19.1 Structured Logging

Branchly emits structured log entries for every significant operation. The log format is JSON-compatible and includes:

```json
{
  "level": "info",
  "event": "commit",
  "namespace": "user_abc123/my-project",
  "sha": "a1b2c3d4",
  "duration_ms": 87,
  "files_changed": 3,
  "bytes_written": 4096
}
```

The logger is injected at construction time. Branchly ships a default no-op logger and a `console` logger for development.

### 19.2 OpenTelemetry Tracing

Every public API call creates an OpenTelemetry span. Storage adapter operations create child spans. This provides end-to-end trace visibility from API call through storage:

```
repo.commit (87ms)
  ├── odb.readObject [HEAD ref] (8ms)
  ├── odb.readObject [tree] (11ms)
  ├── odb.writeObject [blob x3] (21ms)
  ├── odb.writeObject [tree x2] (15ms)
  ├── odb.writeObject [commit] (8ms)
  └── refdb.compareAndSwap [HEAD] (24ms)
```

### 19.3 Metrics

Branchly exposes the following metrics, compatible with Prometheus and OpenTelemetry Metrics:

| Metric | Type | Description |
|---|---|---|
| `branchly.commits.total` | Counter | Total commits created |
| `branchly.commits.duration_ms` | Histogram | Commit operation latency |
| `branchly.objects.written.bytes` | Counter | Total bytes written to ODB |
| `branchly.storage.operations.total` | Counter | Storage adapter calls, by operation |
| `branchly.storage.errors.total` | Counter | Storage adapter errors, by type |
| `branchly.http.requests.total` | Counter | HTTP handler requests, by service |
| `branchly.cache.hit_ratio` | Gauge | Object cache hit ratio |

---

## 20. Versioning & Compatibility

### 20.1 Package Versioning

Branchly follows semantic versioning. Breaking changes to the public API require a major version bump. The public API is defined as all exported classes, methods, and TypeScript types in the `branchly` package.

Storage format changes (e.g., new object addressing scheme, new metadata schema) are considered breaking if they prevent an older version of the library from reading repos created by a newer version.

### 20.2 Storage Format Versioning

Every repo's metadata file (`.branchly/meta.json`) includes a `formatVersion` field. Branchly checks this field on `getRepo()` and rejects repos with a format version higher than its own `MAX_SUPPORTED_FORMAT_VERSION`. Migration tools are provided for upgrades.

### 20.3 Wire Protocol Compatibility

The HTTP smart protocol implementation targets Git protocol v2, with fallback to protocol v0/v1 for older clients. Protocol version negotiation follows the Git specification.

---

## 21. Open Questions

**OQ-1: SHA-1 vs SHA-256 as default.** Git is transitioning from SHA-1 to SHA-256. Defaulting to SHA-256 is forward-looking but means repos created by Branchly cannot be directly pushed to older SHA-1-only remote servers (e.g., older self-hosted GitLab). Should Branchly default to SHA-1 for maximum compatibility, offer SHA-256 as opt-in, or provide transparent translation? Resolution pending community input.

**OQ-2: Pack-file cache invalidation granularity.** The current design invalidates pack caches on any push to the repo. This is safe but aggressive — a push to `feature/x` invalidates the cached pack for `main`. Should we implement per-branch pack caches? The implementation complexity is significant.

**OQ-3: Garbage collection protocol.** Deleted branches leave orphaned objects in the ODB that are never collected. For long-lived repos this could result in significant storage waste. A background GC worker is needed. The design of the GC (what constitutes a GC root, retention policy, incremental vs full) is deferred and warrants its own RFC.

**OQ-4: CAS on S3 — DynamoDB lock backend.** The S3 CAS approximation is theoretically unsafe under very high write contention. Should the `S3Adapter` require an optional DynamoDB lock table configuration for safety, or is the ETag-based approach sufficient for the target use case?

**OQ-5: Signed commits.** Several security-conscious users have requested commit signature verification (GPG or SSH). The implementation would require integrating a signing library and providing a key management story. This is deferred but should be addressed before v1.0.

---

## 22. Alternatives Considered

### 22.1 Shell-out to Git CLI

**Approach:** Use Node.js `child_process.execFile` to call the system `git` binary, parsing stdout/stderr.

**Rejected because:** Requires `git` installed in the deployment environment (breaks edge runtimes and serverless). No typed output — all parsing is string manipulation. Process spawning overhead (30–100ms per call). Cannot operate on non-filesystem storage. Tenant isolation is extremely difficult to enforce. Not testable without a real filesystem.

### 22.2 libgit2 via Node.js native bindings (nodegit / @napi-rs/git)

**Approach:** Wrap the C library `libgit2` using N-API bindings.

**Rejected because:** Requires native compilation at install time, breaking environments without build tools (many CI systems, cloud functions). Binaries must be recompiled per platform/architecture. `nodegit` in particular has a history of installation failures and long build times. Does not support non-filesystem storage without significant custom patches to libgit2.

### 22.3 isomorphic-git as a hard dependency

**Approach:** Re-export `isomorphic-git` directly as the Branchly API.

**Rejected because:** `isomorphic-git`'s API is low-level and closely mirrors Git's own conceptual model, requiring callers to understand objects, trees, and refs. Our goal is to hide this complexity. Additionally, `isomorphic-git` uses a filesystem abstraction based on Node's `fs` interface, which requires wrapping to adapt to arbitrary KV stores. Branchly uses isomorphic-git's core algorithms internally but provides its own API surface.

### 22.4 Database-backed version control (event sourcing approach)

**Approach:** Represent version history as append-only event log in a relational or document database, rather than implementing the Git object model.

**Rejected because:** Abandons Git wire compatibility — repos cannot be cloned by standard Git clients. Does not benefit from Git's efficient pack format. Loses the content-addressability property. Creates a proprietary format with no ecosystem of tooling.

---

## 23. References

- [Git Object Format Specification](https://git-scm.com/book/en/v2/Git-Internals-Git-Objects)
- [Git Pack Format](https://git-scm.com/docs/pack-format)
- [Git HTTP Backend Protocol](https://www.git-scm.com/docs/http-backend)
- [Git Protocol v2](https://git-scm.com/docs/protocol-v2)
- [isomorphic-git](https://isomorphic-git.org) — pure-JS Git implementation referenced in §8.1
- [Gitaly Architecture](https://gitlab.com/gitlab-org/gitaly) — prior art: GitLab's Git RPC service
- [go-git](https://github.com/go-git/go-git) — prior art: pure-Go Git implementation
- [The SHA-256 transition in Git](https://git-scm.com/docs/hash-function-transition) — background for OQ-1
- [RFC 2119](https://www.rfc-editor.org/rfc/rfc2119) — key words for use in RFCs

---

*This document is a living RFC. Sections marked with open questions are subject to revision based on community discussion. To propose changes, open a pull request against the `rfcs/` directory in the Branchly repository.*